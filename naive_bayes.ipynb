{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import unicodecsv\n",
    "except:\n",
    "    !pip install unicodecsv\n",
    "    import unicodecsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy\n",
    "except:\n",
    "    !pip install numpy\n",
    "    import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    !pip install nltk\n",
    "    import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file train_set_x.csv with 276517 rows\n['18', 'Was f\\xc3\\xbcr eine Mail Was soll da drin stehen']\n"
     ]
    }
   ],
   "source": [
    "#This will load the csv\n",
    "import csv\n",
    "def loadDatasetCsv(filename):\n",
    "    info = open(filename, \"rb\")\n",
    "    has_header = csv.Sniffer().has_header(info.read(1024))\n",
    "    info.seek(0)\n",
    "    incsv = csv.reader(info)\n",
    "    if has_header:\n",
    "        next(incsv)  #Skip header\n",
    "    dataset = list(incsv)\n",
    "    return dataset\n",
    "filename = 'train_set_x.csv'\n",
    "dataset_without_classes = loadDatasetCsv(filename)\n",
    "print('Loaded data file {0} with {1} rows').format(filename, len(dataset_without_classes))\n",
    "print(dataset_without_classes[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file train_set_y.csv with 276517 rows\n['18', '3']\n"
     ]
    }
   ],
   "source": [
    "#This will load the csv\n",
    "import csv\n",
    "def loadClassCsv(filename):\n",
    "    info = open(filename, \"rb\")\n",
    "    has_header = csv.Sniffer().has_header(info.read(1024))\n",
    "    info.seek(0)\n",
    "    incsv = csv.reader(info)\n",
    "    if has_header:\n",
    "        next(incsv)  #Skip header\n",
    "    dataset = list(incsv)\n",
    "    return dataset\n",
    "filename = 'train_set_y.csv'\n",
    "classds = loadClassCsv(filename)\n",
    "print('Loaded data file {0} with {1} rows').format(filename, len(classds))\n",
    "print(classds[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Id from list object\n",
    "def getId(line):\n",
    "    info = csv.reader(line, delimiter=',')\n",
    "    new_line = list(info)\n",
    "    return new_line[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Utterance from list object\n",
    "def getUtt(line):\n",
    "    info = csv.reader(line, delimiter=',')\n",
    "    new_line = list(info)\n",
    "    return new_line[1]\n",
    "#print(getUtt(dataset[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClass(line):\n",
    "    info = csv.reader(line, delimiter=',')\n",
    "    new_line = list(info)\n",
    "    new_line_item = ' '.join(new_line[1])\n",
    "    return new_line_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', 'Pas mal pour une ligne Tu sugg\\xc3\\xa8res que Collard choisit', '1']\n"
     ]
    }
   ],
   "source": [
    "def appendClass(dataset):\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i].append(getClass(classds[i])) #Can assume this since they're all in the same position\n",
    "    return dataset\n",
    "dataset_with_classes = appendClass(dataset_without_classes)\n",
    "print(dataset_with_classes[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate by class\n",
    "#Assuming last index has the class nb\n",
    "def separateByClass(dataset):\n",
    "    separated = {}\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        if (int(vector[-1]) not in separated):\n",
    "            separated[int(vector[-1])] = []\n",
    "        separated[int(vector[-1])].append(vector)\n",
    "    return separated\n",
    "\n",
    "separated = separateByClass(dataset_with_classes)\n",
    "print(separated[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBigrams(ds, n):\n",
    "    text = getUtt(ds[n])\n",
    "    vector = ds[n]\n",
    "    class1 = vector[-1]\n",
    "    textNeeded = ''.join(text)\n",
    "    textNeeded = textNeeded.replace(\" \",\"\")\n",
    "    n = 2\n",
    "    bigrams = [[(textNeeded[i:i+n]),class1] for i in range(0,len(textNeeded), 1)]\n",
    "    return bigrams\n",
    "all_ngrams = []\n",
    "for i in range(len(dataset_with_classes)):\n",
    "    all_ngrams.extend(createBigrams(dataset_with_classes,i))\n",
    "print(all_ngrams_with_classes[34])\n",
    "# for i in range(len(all_ngrams)):\n",
    "#     if \"Ã©\" in all_ngrams[i]:\n",
    "#         print(all_ngrams[i])\n",
    "#         del all_ngrams[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatePrior(classDataset, language):\n",
    "    print(\"Calculating prior\")\n",
    "    sum = len(classDataset)\n",
    "    occurence = 0\n",
    "    for i in range(sum):\n",
    "        if(getClass(classDataset[i]) == language):\n",
    "            occurence = occurence + 1\n",
    "    prior = occurence/float(sum)\n",
    "    return prior\n",
    "# nb = calculatePrior(classds, '2')\n",
    "# print(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateLikelihood(all_seq, data, class_wanted):\n",
    "    print(\"Calculating likelihood\")\n",
    "    separated = separateByClass(all_seq)\n",
    "    sum = len(separated[class_wanted])\n",
    "    arr = separated[class_wanted]\n",
    "    occurence = 0\n",
    "    for i in range(sum):\n",
    "        if data in arr[i]:\n",
    "            occurence += 1\n",
    "    h = occurence/float(sum)\n",
    "    return h\n",
    "    \n",
    "# nb = calculateLikelihood(all_ngrams, \"er\", 1)\n",
    "# print(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_a_bigram(bigram):\n",
    "    arr_of_possibilites = [0,0,0,0,0]\n",
    "    arr_of_langs = [\"Slovak\",\"French\",\"Spanish\",\"German\",\"Polish\"]\n",
    "    for i in range(len(arr_of_possibilites)):\n",
    "        print(\"Calculation of stats for \" + arr_of_langs[i])\n",
    "        prior = calculatePrior(classds, str(i))\n",
    "        likelihood = calculateLikelihood(all_ngrams, bigram, i)\n",
    "        arr_of_possibilites[i] = prior*likelihood\n",
    "    return numpy.argmax(arr_of_possibilites)\n",
    "#print(classify_a_bigram(\"os\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file test_set_x.csv with 118508 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\nBelow is the traceback from this internal error.\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n  File \"/Users/karinemellata/anaconda/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n  File \"/Users/karinemellata/anaconda/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n    return f(*args, **kwargs)\n  File \"/Users/karinemellata/anaconda/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n  File \"/Users/karinemellata/anaconda/lib/python2.7/inspect.py\", line 1048, in getinnerframes\n    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n  File \"/Users/karinemellata/anaconda/lib/python2.7/inspect.py\", line 1008, in getframeinfo\n    filename = getsourcefile(frame) or getfile(frame)\n  File \"/Users/karinemellata/anaconda/lib/python2.7/inspect.py\", line 453, in getsourcefile\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\nKeyboardInterrupt\n\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def loadDatasetCsv(filename):\n",
    "    info = open(filename, \"rb\")\n",
    "    has_header = csv.Sniffer().has_header(info.read(1024))\n",
    "    info.seek(0)\n",
    "    incsv = csv.reader(info)\n",
    "    if has_header:\n",
    "        next(incsv)  #Skip header\n",
    "    dataset = list(incsv)\n",
    "    return dataset\n",
    "filename = 'test_set_x.csv'\n",
    "dataset_test_set = loadDatasetCsv(filename)\n",
    "print('Loaded data file {0} with {1} rows').format(filename, len(dataset_test_set))\n",
    "length = len(dataset_test_set)\n",
    "\n",
    "for i in range(length):\n",
    "    ngrams = createBigrams(dataset_test_set,i)\n",
    "    array_of_languages = []\n",
    "    for i in range(len(ngrams)):\n",
    "        array_of_languages.append(classify_a_bigram(ngrams[i]))\n",
    "    print(array_of_languages)\n",
    "    with open('output.csv', \"wb\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        header = \"Id,Category\"\n",
    "        writer.writerow([header])\n",
    "        writer.writerow(getId(dataset_test_set[i]) + dataset_test_set[1])\n",
    "    \n",
    "# ngrams = createBigrams(dataset_test_set,3)\n",
    "# print(ngrams)\n",
    "# all_ngrams_with_classes = []\n",
    "# for i in range(len(dataset_test_set)):\n",
    "#     all_ngrams_with_classes.extend(createBigrams(dataset_test_set,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}