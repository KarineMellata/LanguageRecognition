{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import unicodecsv\n",
    "except:\n",
    "    !pip install unicodecsv\n",
    "    import unicodecsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy\n",
    "except:\n",
    "    !pip install numpy\n",
    "    import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    !pip install nltk\n",
    "    import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file train_set_x.csv with 276517 rows\n",
      "['18', 'Was f\\xc3\\xbcr eine Mail Was soll da drin stehen']\n"
     ]
    }
   ],
   "source": [
    "#This will load the csv\n",
    "import csv\n",
    "def loadDatasetCsv(filename):\n",
    "    info = open(filename, \"rb\")\n",
    "    has_header = csv.Sniffer().has_header(info.read(1024))\n",
    "    info.seek(0)\n",
    "    incsv = csv.reader(info)\n",
    "    if has_header:\n",
    "        next(incsv)  #Skip header\n",
    "    dataset = list(incsv)\n",
    "    return dataset\n",
    "filename = 'train_set_x.csv'\n",
    "dataset_without_classes = loadDatasetCsv(filename)\n",
    "print('Loaded data file {0} with {1} rows').format(filename, len(dataset_without_classes))\n",
    "print(dataset_without_classes[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file train_set_y.csv with 276517 rows\n",
      "['18', '3']\n"
     ]
    }
   ],
   "source": [
    "#This will load the csv\n",
    "import csv\n",
    "def loadClassCsv(filename):\n",
    "    info = open(filename, \"rb\")\n",
    "    has_header = csv.Sniffer().has_header(info.read(1024))\n",
    "    info.seek(0)\n",
    "    incsv = csv.reader(info)\n",
    "    if has_header:\n",
    "        next(incsv)  #Skip header\n",
    "    dataset = list(incsv)\n",
    "    return dataset\n",
    "filename = 'train_set_y.csv'\n",
    "classds = loadClassCsv(filename)\n",
    "print('Loaded data file {0} with {1} rows').format(filename, len(classds))\n",
    "print(classds[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get Id from list object\n",
    "def getId(line):\n",
    "    info = csv.reader(line, delimiter=',')\n",
    "    new_line = list(info)\n",
    "    return new_line[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get Utterance from list object\n",
    "def getUtt(line):\n",
    "    info = csv.reader(line, delimiter=',')\n",
    "    new_line = list(info)\n",
    "    return new_line[1]\n",
    "#print(getUtt(dataset[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getClass(line):\n",
    "    info = csv.reader(line, delimiter=',')\n",
    "    new_line = list(info)\n",
    "    new_line_item = ' '.join(new_line[1])\n",
    "    return new_line_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', 'Pas mal pour une ligne Tu sugg\\xc3\\xa8res que Collard choisit', '1']\n"
     ]
    }
   ],
   "source": [
    "def appendClass(dataset):\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i].append(getClass(classds[i])) #Can assume this since they're all in the same position\n",
    "    return dataset\n",
    "dataset_with_classes = appendClass(dataset_without_classes)\n",
    "print(dataset_with_classes[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separate by class\n",
    "#Assuming last index has the class nb\n",
    "def separateByClass(dataset):\n",
    "    separated = {}\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        if (int(vector[-1]) not in separated):\n",
    "            separated[int(vector[-1])] = []\n",
    "        separated[int(vector[-1])].append(vector)\n",
    "    return separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eq', '1']\n"
     ]
    }
   ],
   "source": [
    "def createBigrams(ds, n):\n",
    "    text = getUtt(ds[n])\n",
    "    vector = ds[n]\n",
    "    class1 = vector[-1]\n",
    "    textNeeded = ''.join(text)\n",
    "    textNeeded = textNeeded.replace(\" \",\"\")\n",
    "    n = 2\n",
    "    bigrams = [[(textNeeded[i:i+n]),class1] for i in range(0,len(textNeeded), 1)]\n",
    "    return bigrams\n",
    "\n",
    "all_ngrams = []\n",
    "for i in range(len(dataset_with_classes)):\n",
    "    all_ngrams.extend(createBigrams(dataset_with_classes,i))\n",
    "print(all_ngrams[34])\n",
    "\n",
    "separated = separateByClass(all_ngrams)\n",
    "# for i in range(len(all_ngrams)):\n",
    "#     if \"Ã©\" in all_ngrams[i]:\n",
    "#         print(all_ngrams[i])\n",
    "#         del all_ngrams[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating prior\n",
      "0.0509154952498\n"
     ]
    }
   ],
   "source": [
    "def calculatePrior(classDataset, language):\n",
    "    print(\"Calculating prior\")\n",
    "    sum = len(classDataset)\n",
    "    occurence = 0\n",
    "    for i in range(sum):\n",
    "        if(getClass(classDataset[i]) == language):\n",
    "            occurence = occurence + 1\n",
    "    prior = occurence/float(sum)\n",
    "    return prior\n",
    "nb = calculatePrior(classds, '4')\n",
    "print(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateLikelihood(all_seq, data, class_wanted):\n",
    "    #print(\"Calculating likelihood\")\n",
    "    arr = separated[class_wanted]\n",
    "    occurence = 0\n",
    "    for i in range(len(separated[class_wanted])):\n",
    "        if data in arr[i]:\n",
    "            occurence += 1\n",
    "    h = occurence/float(len(separated[class_wanted]))\n",
    "    return h\n",
    "    \n",
    "# nb = calculateLikelihood(all_ngrams, \"er\", 1)\n",
    "# print(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This isn't getting used\n",
    "def classify_a_bigram(bigram):\n",
    "    arr_of_possibilities = [0,0,0,0,0]\n",
    "    #Hardcoded the prior calculated above because my computer slow AF\n",
    "    prior_arr = [0.0512337396977,0.510937844689,0.253054965879,0.133857954484,0.0509154952498]\n",
    "    arr_of_langs = [\"Slovak\",\"French\",\"Spanish\",\"German\",\"Polish\"]\n",
    "    for i in range(len(arr_of_possibilities)):\n",
    "        #print(\"Calculation of stats for \" + arr_of_langs[i])\n",
    "        prior = prior_arr[i]\n",
    "        likelihood = calculateLikelihood(all_ngrams, bigram, i)\n",
    "        arr_of_possibilities[i] = prior*likelihood\n",
    "    if arr_of_possibilities == [0,0,0,0,0]:\n",
    "        lang = 1\n",
    "    else:\n",
    "        lang = numpy.argmax(arr_of_possibilities)\n",
    "    return lang\n",
    "#print(classify_a_bigram(\"vp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file test_set_x.csv with 118508 rows\n",
      "We are at the sentence 0 on 118508\n",
      "[]\n",
      "[0.007956470243629093]\n",
      "[0.007956470243629093, 0.008656919804892959]\n",
      "[0.007956470243629093, 0.008656919804892959, 0.0017367965257701762]\n",
      "[0.007956470243629093, 0.008656919804892959, 0.0017367965257701762, 0.005103730212299895]\n",
      "1.70480326411e-12\n",
      "2.1343833453e-12\n",
      "4.40503665e-13\n",
      "1.15590015592e-13\n",
      "2.64319139504e-13\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def loadDatasetCsv(filename):\n",
    "    info = open(filename, \"rb\")\n",
    "    has_header = csv.Sniffer().has_header(info.read(1024))\n",
    "    info.seek(0)\n",
    "    incsv = csv.reader(info)\n",
    "    if has_header:\n",
    "        next(incsv)  #Skip header\n",
    "    dataset = list(incsv)\n",
    "    return dataset\n",
    "filename = 'test_set_x.csv'\n",
    "dataset_test_set = loadDatasetCsv(filename)\n",
    "print('Loaded data file {0} with {1} rows').format(filename, len(dataset_test_set))\n",
    "#length = len(dataset_test_set)\n",
    "likelihood = {}\n",
    "likelihood[0] = []\n",
    "likelihood[1] = []\n",
    "likelihood[2] = []\n",
    "likelihood[3] = []\n",
    "likelihood[4] = []\n",
    "#Hardcoded the prior calculated above because my computer slow AF\n",
    "prior_arr = [0.0512337396977,0.510937844689,0.253054965879,0.133857954484,0.0509154952498]\n",
    "with open('output.csv', \"wb\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    header = \"Id\"\n",
    "    category = \"Category\"\n",
    "    writer.writerow([header] + [category])\n",
    "    for i in range(length):\n",
    "        ngrams = createBigrams(dataset_test_set,i)\n",
    "        ngrams = ['je','te','tu','no','us']\n",
    "        print(\"We are at the sentence \" + str(i) + \" on 118508\")\n",
    "        for j in range(len(ngrams)):\n",
    "            print(likelihood[0])\n",
    "            likelihood[0].append(calculateLikelihood(all_ngrams, ngrams[j], 0))\n",
    "            likelihood[1].append(calculateLikelihood(all_ngrams, ngrams[j], 1))\n",
    "            likelihood[2].append(calculateLikelihood(all_ngrams, ngrams[j], 2))\n",
    "            likelihood[3].append(calculateLikelihood(all_ngrams, ngrams[j], 3))\n",
    "            likelihood[4].append(calculateLikelihood(all_ngrams, ngrams[j], 4))\n",
    "        #array_of_languages.append(classify_a_bigram(ngrams[j]))\n",
    "        k=0\n",
    "        array_of_languages = []\n",
    "        while k < 5:\n",
    "            prod = numpy.prod(likelihood[k])\n",
    "            #Bayesian for each language\n",
    "            array_of_languages.append(prod*prior_arr[k])\n",
    "            k = k + 1\n",
    "        #Winning language\n",
    "        language = str(numpy.argmax(array_of_languages))\n",
    "        writer.writerow(getId(dataset_test_set[i]) + [language])\n",
    "        \n",
    "# ngrams = createBigrams(dataset_test_set,3)\n",
    "# print(ngrams)\n",
    "# all_ngrams_with_classes = []\n",
    "# for i in range(len(dataset_test_set)):\n",
    "#     all_ngrams_with_classes.extend(createBigrams(dataset_test_set,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
