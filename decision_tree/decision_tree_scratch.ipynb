{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk # natural language processing\n",
    "import csv # needed to load csv files\n",
    "import numpy as np\n",
    "import pandas as pd # for creation of dataframes later\n",
    "import pickle # for saving lists\n",
    "import re\n",
    "import random # needed to randomly select characters from string\n",
    "from collections import Counter # to count vocabulary\n",
    "\n",
    "arr_of_langs = [\"Slovak\",\"French\",\"Spanish\",\"German\",\"Polish\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function to load csv files and remove headers\n",
    "def LoadCSV(filename):\n",
    "    raw_file = open(filename, encoding=\"utf8\")\n",
    "\n",
    "    # determine if the file has a header\n",
    "    has_header = csv.Sniffer().has_header(raw_file.read(1024))\n",
    "    raw_file.seek(0)\n",
    "    \n",
    "    # read the csv and make the data a list\n",
    "    incsv = csv.reader(raw_file)\n",
    "    data = list(incsv)\n",
    "    \n",
    "    # remove the first row if there is a header\n",
    "    if has_header:\n",
    "        data = data[1:]\n",
    "    \n",
    "    return data\n",
    "\n",
    "# load the training set x\n",
    "x_filename = '../data/train_set_x.csv'\n",
    "x_values = LoadCSV(x_filename)\n",
    "print(len(x_values))\n",
    "print(x_values[18])\n",
    "\n",
    "# And the training set y\n",
    "y_filename = '../data/train_set_y.csv'\n",
    "y_values = LoadCSV(y_filename)\n",
    "print(len(y_values))\n",
    "print(y_values[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a vector of the classes (y_values) to the data (x_values)\n",
    "\n",
    "just_class = [x[1] for x in y_values]\n",
    "\n",
    "def append(dataset, classes):\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i].append(classes[i])\n",
    "    return dataset\n",
    "\n",
    "full_data = append(x_values, just_class)\n",
    "just_text = [x[1] for x in full_data]\n",
    "print(full_data[20:25])\n",
    "print(just_text[20:25])\n",
    "type(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to make the data comparable to the test set by sampling 2 letters from each word\n",
    "# and creating a just_letters list that we can then produce a vector space model for to use in a decision-tree\n",
    "\n",
    "list_of_characters = []\n",
    "number_of_words = []\n",
    "random_characters = []\n",
    "\n",
    "def convert_to_letters(dataset):\n",
    "    for sentence in dataset:\n",
    "        characters = []\n",
    "        for c in sentence.lower():\n",
    "            if not c == ' ':\n",
    "                characters.append(c)\n",
    "        \n",
    "        # add random characters drawn from each string. we draw 2 times the number of words in the sentence.\n",
    "        random_characters.append(random.choices(characters, k=(len(sentence.split())*2)))\n",
    "    \n",
    "    return random_characters\n",
    "\n",
    "character_data = convert_to_letters(just_text)\n",
    "print(character_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to create a listing of all the characters that appear - we will call this character_list\n",
    "type(character_data)\n",
    "\n",
    "character_list = list(set([element for tupl in character_data for element in tupl]))\n",
    "len(character_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector space model with character_list parameters (424) for the training set - this takes some time\n",
    "doc_term_matrix = []\n",
    "\n",
    "def tf(term, document):\n",
    "  return freq(term, document)\n",
    "\n",
    "def freq(term, document):\n",
    "  return document.count(term)\n",
    "\n",
    "for count, doc in enumerate(character_data):\n",
    "    tf_vector = [tf(character, doc) for character in character_list]\n",
    "    tf_vector_string = ', '.join(format(freq, 'd') for freq in tf_vector)\n",
    "    doc_term_matrix.append(tf_vector)\n",
    "    \n",
    "    if count % 10000 == 0:\n",
    "        print(count)   \n",
    "        \n",
    "print(doc_term_matrix[:10])\n",
    "len(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE FOLLOWING IS DERIVED FROM THE FOLLOWING GUIDES\n",
    "# http://kldavenport.com/pure-python-decision-trees/\n",
    "# https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/\n",
    "\n",
    "# BUT MOSTLY FROM THE BASE CODE FROM: \n",
    "# https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb\n",
    "\n",
    "# I have annotated the code clearly and adjusted where appropriate to more accurately meet the needs of the assignment, \n",
    "# but the skeleton of the core code (Questions, gini, info_gain, partition, find_best_split, and build_tree functions)\n",
    "# is drawn from random-forests. I have cited them in the report.\n",
    "\n",
    "vector_space = doc_term_matrix\n",
    "header = character_list\n",
    "\n",
    "print(len(vector_space)) # should be ~270000\n",
    "print(len(just_class)) # should be ~270000\n",
    "print(len(header)) # should be 424\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT THE OUTPUTS FROM THE INITIAL vector_space PYTHON NOTEBOOK\n",
    "# These are the vector_space, the classes of the training data, and the full character list being evaluated upon\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# open_name = '../data/doc_term_matrix.txt'\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# we created our vector space model and training classes in file \"vector_space\"\n",
    "\n",
    "# first load up the data\n",
    "# vector_space = pickle.load(open(open_name, \"rb\"))   \n",
    "# just_class = pickle.load(open(\"../data/just_class.txt\", \"rb\"))\n",
    "# header = pickle.load(open('../data/character_list.txt', \"rb\"))\n",
    "# len(just_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPEND THE DATA AND CLASS TOGETHER FOR OUR TREE\n",
    "\n",
    "def append(dataset, classes):\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i].append(classes[i])\n",
    "    return dataset\n",
    "\n",
    "full_data = append(vector_space, just_class)\n",
    "\n",
    "# show all the different characters I will use for the vector space model\n",
    "print(header)\n",
    "print(len(full_data))\n",
    "print(full_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO DETERMINE THE SIZE OF EACH CLASS\n",
    "\n",
    "def class_counts(rows):\n",
    "    counts = {}  # a dictionary that will contain label and count\n",
    "    for row in rows:\n",
    "        # the class is the last column in our data\n",
    "        label = row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "\n",
    "# this shows the number for each class\n",
    "class_counts(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CLASS QUESTION TO PARTITION THE DATASET BY A SPECIFIC QUESTION\n",
    "\n",
    "class Question:\n",
    "    \n",
    "    def __init__(self, column, value):\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "\n",
    "    def match(self, example):\n",
    "        # Compare the feature value in an example to the\n",
    "        # feature value in this question.\n",
    "        val = example[self.column]\n",
    "        return val >= self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CALCULATE THE UNCERTAINTY (GINI) IN THE DATA\n",
    "\n",
    "def gini(rows):\n",
    "\n",
    "    counts = class_counts(rows)\n",
    "    impurity = 1\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        impurity -= prob_of_lbl**2\n",
    "    return impurity\n",
    "\n",
    "current_uncertainty = gini(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNCTION TO CALCULATE THE INFORMATION GAIN FROM A SPECIFIC QUESTION\n",
    "\n",
    "def info_gain(left, right, current_uncertainty):\n",
    "    p = float(len(left)) / (len(left) + len(right))\n",
    "    return current_uncertainty - p * gini(left) - (1 - p) * gini(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO PARTITION A DATASET. BASED ON A SPECIFIC QUESTION BREAKS INTO THE BRANCHES\n",
    "\n",
    "def partition(rows, question):\n",
    "    \n",
    "    true_rows, false_rows = [], []\n",
    "    for row in rows:\n",
    "        if question.match(row):\n",
    "            true_rows.append(row)\n",
    "        else:\n",
    "            false_rows.append(row)\n",
    "    return true_rows, false_rows\n",
    "\n",
    "true_rows, false_rows = partition(full_data, Question(10, 1))\n",
    "info_gain(true_rows, false_rows, current_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FIND THE BEST QUESTION TO ASK BY CHECKING EVERY FEATURE-VALUE PAIR AND CALCULATING INFORMATION GAIN\n",
    "\n",
    "def find_best_split(rows):\n",
    "\n",
    "    best_gain = 0  # start empty but will be filled by the question that does the best\n",
    "    best_question = None  # track the question\n",
    "    current_uncertainty = gini(rows) # get our uncertainty value (in case it is not already calculated)\n",
    "    n_features = len(rows[0]) - 1  # number of columns in the data (-1 because of the value)\n",
    "\n",
    "    for col in range(n_features):  # for each column/feature\n",
    "\n",
    "        values = set([row[col] for row in rows])  # get the unique values\n",
    "\n",
    "        for val in values:  # for each unique value\n",
    "\n",
    "            question = Question(col, val) # confirm the question that will be the most useful\n",
    "\n",
    "            # try splitting the dataset\n",
    "            # if it does not divide the dataset (i.e. all true or all false, then skip it)\n",
    "            true_rows, false_rows = partition(rows, question) \n",
    "            if len(true_rows) == 0 or len(false_rows) == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate the information gain from this split\n",
    "            gain = info_gain(true_rows, false_rows, current_uncertainty)\n",
    "\n",
    "            # If this gain is the best we have found, then save it \n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_question = question\n",
    "\n",
    "    return best_gain, best_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ADD A NEW CLASS LEAF - HOLDS THE RELATIONSHIP BETWEEN A CLASS AND A LEAF\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, rows):\n",
    "        self.predictions = class_counts(rows)\n",
    "        \n",
    "# ADD A NEW DECISION NODE CLASS. Find the question and the two associated branches.\n",
    "\n",
    "class Decision_Node:\n",
    "\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 true_branch,\n",
    "                 false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FINALLY, WE CAN BUILD THE TREE\n",
    "\n",
    "# this is a recursive tree that only stops when it finds no new information gain...we want to limit the number of max_features\n",
    "\n",
    "def build_tree(rows, count):\n",
    "    \n",
    "    count = count + 1\n",
    "    # First we find the best question that we can ask\n",
    "    gain, question = find_best_split(rows)\n",
    "\n",
    "    # When we do not gain any more information, this is a leaf and ends that line of inquiry\n",
    "    # could tweak this parameter to only get things with better gains...this is an the intensive task, however.\n",
    "    if gain == 0:\n",
    "        return Leaf(rows)\n",
    "    \n",
    "    # this also ends if we are at the fifth iteration of finding true or false branches\n",
    "    if count == 5:\n",
    "        print(\"count is at 5 - creating a leaf...\")\n",
    "        count = 0\n",
    "        return Leaf(rows)\n",
    "\n",
    "    # Assuming we do not have a leaf, then there is a useful feature of the data that we can partition on\n",
    "    true_rows, false_rows = partition(rows, question)\n",
    "\n",
    "    # Start the recursion - build a branch for the trues\n",
    "    true_branch = build_tree(true_rows, count)\n",
    "\n",
    "    # and for the falses...\n",
    "    false_branch = build_tree(false_rows, count)\n",
    "\n",
    "    # Return a Question node. We now have our best feature/value to ask.\n",
    "    # Note that this may overfit the data as it always looks for more information gain...\n",
    "    # I should add something to limit it...\n",
    "    return Decision_Node(question, true_branch, false_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "time = datetime.datetime.now()\n",
    "\n",
    "# BUILD THE TREE - THIS TAKES A LONG TIME WITH THE FULL DATA AND DOES NOT\n",
    "# SIGNIFICANTLY INCREASE PERFORMANCE. I RECOMMEND CHANGING TO USING ONLY A PORTION OF THE TRAINING SET\n",
    "# OR A CROSS-VALIDATION TECHNIQUE\n",
    "\n",
    "# decision_tree_10 = build_tree(full_data[:10], 0)\n",
    "time =  datetime.datetime.now()-time\n",
    "print(time)\n",
    "time = datetime.datetime.now()\n",
    "\n",
    "# decision_tree_100 = build_tree(full_data[:100],0)\n",
    "time = datetime.datetime.now()-time\n",
    "print(time)\n",
    "time = datetime.datetime.now()\n",
    "\n",
    "# decision_tree_1000 = build_tree(full_data[:1000],0)\n",
    "time = datetime.datetime.now()-time\n",
    "print(time)\n",
    "time = datetime.datetime.now()\n",
    "\n",
    "# decision_tree_10000 = build_tree(full_data[:10000],0)\n",
    "time = datetime.datetime.now()-time\n",
    "print(time)\n",
    "time = datetime.datetime.now()\n",
    "\n",
    "# decision_tree_100000 = build_tree(full_data[:100000],0)\n",
    "time = datetime.datetime.now()-time\n",
    "print(time)\n",
    "time = datetime.datetime.now()\n",
    "\n",
    "decision_tree_full = build_tree(full_data,0)\n",
    "time = datetime.datetime.now()-time\n",
    "print(time)\n",
    "time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction method\n",
    "\n",
    "def classify(row, node):\n",
    "\n",
    "    # If we have reached a leaf then we use that to determine the class\n",
    "    if isinstance(node, Leaf):\n",
    "        return node.predictions\n",
    "\n",
    "    # If no leaf, then decide whether to follow the true-branch or the false-branch to recursively classify the data\n",
    "    if node.question.match(row):\n",
    "        return classify(row, node.true_branch)\n",
    "    else:\n",
    "        return classify(row, node.false_branch)\n",
    "\n",
    "# CONFRIM THAT CLASSIFY IS WORKING\n",
    "list(classify(full_data[1], decision_tree_10000))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK THE ACCURACY AGAINST ~5% OF THE ORIGINAL DATA\n",
    "prediction_training = []\n",
    "\n",
    "for row in vector_space[263517:]:\n",
    "    prediction_training.append(list(classify(row, decision_tree_full))[0])\n",
    "    \n",
    "prediction_verify = list(zip(prediction_training, just_class[263517:]))\n",
    "\n",
    "# this gives the accuracy of our model on the training data...note that it is very low...\n",
    "print(len([i for i, j in prediction_verify if i == j])/len(just_class[263517:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME TO PRODUCE THE OUTPUT.CSV file\n",
    "# FIRST, LOAD THE TEST DATA\n",
    "import csv\n",
    "\n",
    "# load the test set x\n",
    "x_filename = '../data/test_set_x.csv'\n",
    "x_test = LoadCSV(x_filename)\n",
    "print(len(x_test))\n",
    "print(x_test[400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE THE ID FROM THE TEST DATA\n",
    "just_text = [x[1] for x in x_test]\n",
    "just_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A VECTOR SPACE MODEL FOR THE TEST DATA\n",
    "doc_term_matrix = []\n",
    "\n",
    "for count, doc in enumerate(just_text):\n",
    "    tf_vector = [tf(character, doc) for character in header]\n",
    "    tf_vector_string = ', '.join(format(freq, 'd') for freq in tf_vector)\n",
    "    doc_term_matrix.append(tf_vector)\n",
    "    \n",
    "    if count % 10000 == 0:\n",
    "        print(count)   \n",
    "        \n",
    "print(doc_term_matrix[:1])\n",
    "len(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE PREDICTIONS FOR EACH ROW IN THE doc_term_matrix\n",
    "\n",
    "model_used = decision_tree_full\n",
    "model_name = 'decision_tree_full'\n",
    "\n",
    "prediction = []\n",
    "\n",
    "for row in doc_term_matrix:\n",
    "    prediction.append(list(classify(row, model_used))[0])\n",
    "    \n",
    "prediction[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPEND THE ID TO THE PREDICTION VALUES\n",
    "prediction_id = list(range(0,len(just_text)))\n",
    "prediction_output = list(zip(prediction_id, prediction))\n",
    "\n",
    "prediction_output[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PRODUCE THE CSV IN REQUIRED OUTPUT FORMAT\n",
    "\n",
    "model_string = '../submissions/output_scratch_%s.csv' %model_name\n",
    "\n",
    "with open(model_string, \"w\", newline='') as csvfile:\n",
    "    csv_out=csv.writer(csvfile)\n",
    "    csv_out.writerow(['Id','Category'])\n",
    "    for row in prediction_output:\n",
    "        csv_out.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
